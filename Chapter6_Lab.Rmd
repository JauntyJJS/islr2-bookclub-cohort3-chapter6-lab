---
title: '**Chapter 6 Lab**'
author: "Jeremy Selva"
subtitle: This document was prepared on `r format(Sys.Date())`.
output:
  html_document:
    theme: cerulean
    highlight: pygments
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: true
    number_sections: no
    code_folding: show
    code_download: yes
    self_contained: true
  rmarkdown::html_vignette:
    toc: yes
    toc_depth: 2
bibliography: utils/bibliography.bib
csl: utils/f1000research.csl
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, warning=FALSE, message=TRUE, include=FALSE}
# Set up the environment

# Options relative to figure size
# 1.618 is the golden ratio
figheight <- 4
figwidth <- 4 * 1.618 

# General options
options(knitr.kable.NA = "",
        nsmall = 3,
        tidyverse.quiet = TRUE
        )
hook_output <- knitr::knit_hooks$get('output')

knitr::knit_hooks$set(
  output = function(x, options) {
    if (!is.null(options$max.height)) {
      options$attr.output <- c(options$attr.output,
                               sprintf('style="max-height: %s;"', options$max.height))
    }
    hook_output(x, options)
    }
  )

# Chunk options (see https://yihui.org/knitr/options/#chunk_options)
knitr::opts_chunk$set(
  comment = ">",  # The prefix to be added before each line of the text output.
  dpi = 600,
  fig.path = "img/",
  fig.height = figheight,
  fig.width = figwidth,
  fig.align = "center",
  # See https://community.rstudio.com/t/centering-images-in-blogdown-post/20962
  # to learn how to center images
  # See https://bookdown.org/yihui/rmarkdown-cookbook/opts-tidy.html
  # See https://www.zotero.org/styles for citation style respository
  tidy='styler',
  tidy.opts=list(strict=TRUE)
)
```

```{r warning=FALSE, message=FALSE, results='asis'}
library(leaps)
library(tibble)
library(magrittr)
library(dplyr)
library(tidyr)
library(report)
library(ISLR)

library(reactable)

library(parsnip)
library(glmnet)
library(rsample)
library(recipes)
library(tune)
library(workflows)
library(dials)
library(yardstick)

# For recipes::step_pls
# Warning message:
#  `step_pls()` failed: Error in loadNamespace(x) : there is no package called ‘mixOmics’
# Install from Bioconductor
library(mixOmics)


summary(report::report(sessionInfo()))
```

# Subset Selection Methods

## Best Subset Selection

```{r, echo=FALSE}
set.seed(1234)
```

We will be using the `Hitters` data set from the `ISLR` package. We wish
to predict the baseball players `Salary` based on several different
characteristics which are included in the data set.

Remove all rows with missing data from that column.

```{r, message=FALSE}

Hitters <- as_tibble(ISLR::Hitters) %>%
  tidyr::drop_na()
```

Use `regsubsets()` function to performs best subset selection.

An asterisk indicates that a given variable is included in the
corresponding model. For instance, this output indicates that the best
two-variable model contains only Hits and CRBI.

```{r, message = FALSE, max.height = '150px'}

regfit.full <- leaps::regsubsets(
  x = Salary ~ ., 
  data = Hitters,
  nvmax = 8 #default
  )

summary(regfit.full)
```

Here we fit up to a 19-variable model.

```{r, message=FALSE}

regfit.full <- leaps::regsubsets(
  x = Salary ~ ., 
  data = Hitters,
  nvmax = 19 #default
  )
```

The `summary()` function also returns $R^2$, $RSS$, adjusted $R^2$,
$C_p$, and $BIC$. We can examine these to try to select the best overall
model.

```{r, message=FALSE}

reg.summary <- regfit.full %>%
  summary()

names(reg.summary)
```

For instance, we see that the $R^2$ statistic increases from 32 %, when
only one variable is included in the model, to almost 55 %, when all
variables are included. As expected, the $R^2$ statistic increases
monotonically as more variables are included.

```{r, message=FALSE}

reg.summary$rsq
```

The `regsubsets()` function has a built-in `plot()` command which can be
used to display the selected variables for the best model with a given
number of predictors, ranked according to the $BIC$, $C_p$, adjusted
$R^2$, or $AIC$. To find out more about this function, type
`?plot.regsubsets`.

The top row of each plot contains a black square for each variable
selected according to the optimal model associated with that statistic.
For instance, we see that several models share a $BIC$ close to $−150$.
However, the model with the lowest $BIC$ is the six-variable model that
contains only `AtBat`, `Hits`, `Walks`, `CRBI`, `DivisionW`, and
`PutOuts`.

```{r, message=FALSE}
plot(regfit.full , scale = "bic")
```

We can use the `coef()` function to see the coefficient estimates
associated with this model.

```{r, message=FALSE}
coef(regfit.full , 6)
```

## Forward and Backward Stepwise Selection

We can also use the `regsubsets()` function to perform forward stepwise
selection using the argument `method = "forward"`

```{r, message=FALSE, max.height='150px'}
regfit.fwd <- leaps::regsubsets(
  x = Salary ~ ., 
  data = Hitters ,
  nvmax = 19, 
  method = "forward"
)

summary(regfit.fwd)
```

To perform backward stepwise selection, use the argument
`method = "backward"`

```{r, message=FALSE, max.height='150px'}
regfit.bwd <- leaps::regsubsets(
  x = Salary ~ ., 
  data = Hitters ,
  nvmax = 19, 
  method = "backward"
)

summary(regfit.bwd)
```

For this data, the best seven-variable models identified by forward
stepwise selection, backward stepwise selection, and best subset
selection are different.

```{r, message=FALSE}

coef(regfit.full,7)
coef(regfit.fwd,7)
coef(regfit.bwd,7)

```

## Choosing Among Models Using the Validation-Set Approach

We split the samples into training set and a test set

```{r, message=FALSE}

set.seed(1234)

Hitters <- Hitters %>%
  dplyr::mutate(
    isTrainingSet = sample(x = c(TRUE , FALSE),
                           size = nrow(Hitters),
                           replace = TRUE)
  ) %>%
  dplyr::relocate(.data[["isTrainingSet"]])

train <- Hitters %>%
  dplyr::filter(.data[["isTrainingSet"]] == TRUE) %>%
  dplyr::select(-c("isTrainingSet"))

test <- Hitters %>%
  dplyr::filter(.data[["isTrainingSet"]]== FALSE) %>%
  dplyr::select(-c("isTrainingSet"))

```

Now, we apply `regsubsets()` to the training set in order to perform
best subset selection and compute the the validation set error for the
best model of each model size.

```{r, message=FALSE}

regfit.best <- leaps::regsubsets(Salary ~ ., data = train, nvmax = 19)

test.mat <- stats::model.matrix(Salary ~ ., data = test)

val.errors <- rep(NA, 19)

for (i in 1:19) {
  coefi <- stats::coef(regfit.best , id = i)
  pred <- test.mat[, names(coefi)] %*% coefi
  val.errors[i] <- mean (( test$Salary - pred)^2)
}

val.errors
```

We find that the best model is the one that contains seven variables.

```{r, message=FALSE}
which.min(val.errors)

coef(regfit.best , 7)
```

However, the best seven-variable model on the full data set has a
different set of variables than the best seven-variable model on the
training set.

```{r, message=FALSE}
regfit.best <- leaps::regsubsets(Salary ~ ., data = Hitters , nvmax = 19)
coef(regfit.best , 7)
```

## Choosing Among Models Using Cross-Validation

We now try to choose among the models of different sizes using cross
validation. Create a vector that allocates each observation to one of k
= 10 folds.

```{r, message=FALSE}
set.seed (1)

k <- 10
n <- nrow(Hitters)
folds <- sample(rep (1:k, length = n))

```

We write a for loop that performs cross-validation

```{r, message=FALSE}

# A predict method for regsubsets
predict.regsubsets <- function(object , newdata , id, ...) {
  form <- as.formula(object$call [[2]])
  mat <- model.matrix(form , newdata)
  coefi <- coef(object , id = id)
  xvars <- names(coefi)
  mat[, xvars] %*% coefi
  }

cv.errors <- matrix(NA, k, 19,
                    dimnames = list(NULL , paste (1:19)))
for (j in 1:k) {
  best.fit <- leaps::regsubsets(Salary ~ .,
                                data = Hitters[folds != j,],
                                nvmax = 19)
  for (i in 1:19) {
    pred <- stats::predict(best.fit, Hitters[folds == j, ], id = i)
    cv.errors[j, i] <- mean(( Hitters$Salary[folds == j] - pred)^2)
  }
  }

```

This has given us a $10$ by $19$ matrix, of which the $(j, i)$th element
corresponds to the test $MSE$ for the $j$th cross-validation fold for
the best $i$-variable model.

We use the `apply()` function to average over the columns of this
`apply()` matrix in order to obtain a vector for which the $i$th element
is the cross validation error for the $i$-variable model.

```{r, message=FALSE}

mean.cv.errors <- apply(cv.errors , 2, mean)

plot(mean.cv.errors , type = "b")

```

We see that cross-validation selects a 10-variable model.

```{r, message=FALSE}
regfit.best <- leaps::regsubsets(Salary ~ ., data = Hitters , nvmax = 19)
coef(regfit.best , 10)
```

# Ridge Regression

## Introduction to ridge regression 

Set mixture to 0 to indicate Ridge Regression. For now, we set
penalty/lambda as 0

```{r, message=FALSE}
Hitters <- as_tibble(ISLR::Hitters) %>%
  tidyr::drop_na()

ridge_spec <- parsnip::linear_reg(mixture = 0,
                                  penalty = 0) %>%
  parsnip::set_mode("regression") %>%
  parsnip::set_engine("glmnet")

ridge_spec %>%
  parsnip::translate()
```

Once the specification is created we can fit it to our data. We will use
all the predictors.

```{r, message=FALSE}
ridge_fit <- ridge_spec %>%
  parsnip::fit(formula = Salary ~ ., 
               data = Hitters)
```

We can see the parameter estimate for different values of
penalty/lambda. Notice how the estimates are decreasing when the amount
of penalty goes up

```{r, message=FALSE, max.height='150px'}
parsnip::tidy(ridge_fit)
parsnip::tidy(ridge_fit, penalty = 705)
parsnip::tidy(ridge_fit, penalty = 11498)
```

We can visualize how the magnitude of the coefficients are being
regularized towards zero as the penalty goes up.

```{r, message=FALSE}
# Arrange figure margin
par(mar=c(5,5,5,1))
ridge_fit %>%
  parsnip::extract_fit_engine() %>%
  plot(xvar = "lambda")
```

It would be nice if we could find the "best" value of the
penalty/lambda. We can do this using the `tune::tune_grid()`

Need we need three things in order to use the `tune::tune_grid()`

-   a `resample` object containing the resamples the `workflow` should be
    fitted within,
    
-   a `workflow` object containing the model and preprocessor,and

-   a tibble `penalty_grid` containing the parameter values to be evaluated.

First, we create a 10-fold cross-validation data set from the training
data set.

## Create the resample object

```{r, message=FALSE}
set.seed(1234)
Hitters <- as_tibble(ISLR::Hitters) %>%
  tidyr::drop_na()

Hitters_split <- rsample::initial_split(Hitters, strata = "Salary")

Hitters_train <- rsample::training(Hitters_split)
Hitters_test <- rsample::testing(Hitters_split)

Hitters_fold <- rsample::vfold_cv(Hitters_train, v = 10)

```

## Create the preprocessor

We use the `recipes` package to create the preporcessing steps. However, the order of the preprocessing step is actually important. See the [Ordering of steps vignette](https://cran.r-project.org/web/packages/recipes/vignettes/Ordering.html) 

We create a basic recipe.

```{r, message=FALSE}

ridge_recipe <- recipes::recipe(formula = Salary ~ .,
                                data = Hitters_train)

rec <- recipes::prep(x = ridge_recipe, training = Hitters_train)
  
rec %>%
  summary() %>%
  reactable::reactable(defaultPageSize = 5)

rec %>%
  recipes::bake(new_data = Hitters_train) %>%
  reactable::reactable(defaultPageSize = 5)

```

Do preprocessing on the factor variables.

```{r, message=FALSE}

ridge_recipe <- ridge_recipe %>%
  # Step Novel is important.
  # If test set has a new factor not found in training, it will be treated as "new" and not NA
  # This will prevent the model from giving an error
  # https://blog.datascienceheroes.com/how-to-use-recipes-package-for-one-hot-encoding/
  recipes::step_novel(recipes::all_nominal_predictors()) %>%
  # Create Dummy variables
  recipes::step_dummy(recipes::all_nominal_predictors())

rec <- recipes::prep(x = ridge_recipe, training = Hitters_train)
  
rec %>%
  summary() %>%
  reactable::reactable(defaultPageSize = 5)

rec %>%
  recipes::bake(new_data = Hitters_train) %>%
  reactable::reactable(defaultPageSize = 5)

```

Do normalisation step.

```{r, message=FALSE}

ridge_recipe <- ridge_recipe %>%
  # Remove predictors with zero variation
  recipes::step_zv(recipes::all_predictors()) %>%
  # Standardise all variable
  recipes::step_normalize(recipes::all_predictors())

rec <- recipes::prep(x = ridge_recipe, training = Hitters_train)
  
rec %>%
  recipes::bake(new_data = Hitters_train) %>%
  reactable::reactable(defaultPageSize = 5)


```

## Specify the model

The model specification will look very similar to what we have seen earlier, but we will set `penalty = tune::tune()`. This tells `tune::tune_grid()` that the penalty parameter should be tuned.

```{r, message=FALSE}

ridge_spec <- 
  parsnip::linear_reg(penalty = tune::tune(), mixture = 0) %>% 
  parsnip::set_mode("regression") %>% 
  parsnip::set_engine("glmnet")

ridge_spec %>%
  parsnip::translate()
```

## Create the workflow

```{r, message=FALSE}

ridge_workflow <-  workflows::workflow() %>% 
  workflows::add_recipe(ridge_recipe) %>% 
  workflows::add_model(ridge_spec)

ridge_workflow
```

## Create the penalty/lambda grid

A penalty/lambda grid of $50$ numbers from $0.00001$ ($10^{-5}$) to $10000$ ($10^5$) is created.

```{r, message=FALSE}

# Create a range from 10
penalty_grid <- dials::grid_regular(dials::penalty(range = c(-5, 5)), levels = 50)

penalty_grid  %>% 
  reactable::reactable(defaultPageSize = 5)
```

## Ridge regression model fitting on cross validated data

Now we have everything we need and we can fit all the models on the cross validated data. Note that this process may take some time.

```{r, message=FALSE, max.height='150px'}

tune_res <- tune::tune_grid(
  object = ridge_workflow,
  resamples = Hitters_fold, 
  grid = penalty_grid
)

tune_res
```

Here we see that the amount of regularization affects the performance metrics differently. Do note that using a different seed will give a different plot

```{r, message=FALSE}
# Note that a different seed will give different plots
tune::autoplot(tune_res)
```

We can also see the raw metrics that created this chart by calling `tune::collect_metrics()`.

```{r, message=FALSE, max.height='150px'}

tune::collect_metrics(tune_res) %>% 
  reactable::reactable(defaultPageSize = 5)
```

The “best” values of this can be selected using `tune::select_best()`, this function requires you to specify a metric that it should select against. The penalty/lambda value is 569 for metric `rsq` since it gives the highest value. Do note that using a different seed will give a different penalty/lambda value

```{r, message=FALSE, max.height='150px'}

top_penalty <- tune::show_best(tune_res, metric = "rsq", n = 5)
top_penalty

best_penalty <- tune::select_best(tune_res, metric = "rsq")
best_penalty

```

## Ridge regression model with optimised penalty/lambda value

We create the ridge regression workflow with the best penalty score.

```{r, message=FALSE}

ridge_final <- tune::finalize_workflow(x = ridge_workflow, 
                                       parameters = best_penalty)

ridge_final
```

We now train the ridge regression model with the training data

```{r, message=FALSE}

ridge_final_fit <- parsnip::fit(object = ridge_final, data = Hitters_train)
```

## Ridge regression model on test data

This ridge regression model can now be applied on our testing data set to validate its performance. For regression models, a .pred column is added.

```{r, message=FALSE}

test_results <- parsnip::augment(x = ridge_final_fit, new_data = Hitters_test)
  
test_results %>%
  reactable::reactable(defaultPageSize = 5)
  
```

```{r, message=FALSE}
test_results %>%
  yardstick::rsq(truth = .data[["Salary"]], estimate = .data[[".pred"]]) %>%
  reactable::reactable(defaultPageSize = 5)
```

# The Lasso

The following procedure will be very similar to what we saw in the ridge regression section. The resampling step is the same

## Create the resample object

```{r, message=FALSE}
set.seed(1234)
Hitters <- as_tibble(ISLR::Hitters) %>%
  tidyr::drop_na()

Hitters_split <- rsample::initial_split(Hitters, strata = "Salary")

Hitters_train <- rsample::training(Hitters_split)
Hitters_test <- rsample::testing(Hitters_split)

Hitters_fold <- rsample::vfold_cv(Hitters_train, v = 10)

```

## Create the preprocessor

The following procedure will be very similar to what we saw in the ridge regression section. The preprocessing needed is the same.

```{r, message=FALSE}

lasso_recipe <- 
  recipes::recipe(formula = Salary ~ ., data = Hitters_train) %>% 
  recipes::step_novel(recipes::all_nominal_predictors()) %>% 
  recipes::step_dummy(recipes::all_nominal_predictors()) %>% 
  recipes::step_zv(recipes::all_predictors()) %>% 
  recipes::step_normalize(recipes::all_predictors())
```

## Specify the model

This time, it is `mixture=1`

```{r, message=FALSE}

lasso_spec <- 
  parsnip::linear_reg(penalty = tune::tune(), mixture = 1) %>% 
  parsnip::set_mode("regression") %>% 
  parsnip::set_engine("glmnet")

lasso_spec %>%
  parsnip::translate()

```

## Create the workflow

```{r, message=FALSE}

lasso_workflow <-  workflows::workflow() %>% 
  workflows::add_recipe(lasso_recipe) %>% 
  workflows::add_model(lasso_spec)

lasso_workflow
```

## Create the penalty/lambda grid

A penalty/lambda grid of $50$ numbers from $0.01$ ($10^{-2}$) to $100$ ($10^2$) is created.

```{r, message=FALSE}

# Create a range from 10
penalty_grid <- dials::grid_regular(dials::penalty(range = c(-2, 2)), levels = 50)

penalty_grid  %>% 
  reactable::reactable(defaultPageSize = 5)
```

## Lasso model fitting on cross validated data

Now we have everything we need and we can fit all the models on the cross validated data. Note that this process may take some time.

```{r, message=FALSE, max.height='150px'}

tune_res <- tune::tune_grid(
  object = lasso_workflow,
  resamples = Hitters_fold, 
  grid = penalty_grid
)

tune_res
```

Here we see that the amount of regularization affects the performance metrics differently. Do note that using a different seed will give a different plot

```{r, message=FALSE}
# Note that a different seed will give different plots
tune::autoplot(tune_res)
```


The “best” values of this can be selected using `tune::select_best()`, this function requires you to specify a metric that it should select against. The penalty/lambda value is 22.2 for metric `rsq` since it gives the highest value. Do note that using a different seed will give a different penalty/lambda value

```{r, message=FALSE, max.height='150px'}

top_penalty <- tune::show_best(tune_res, metric = "rsq", n = 5)
top_penalty

best_penalty <- tune::select_best(tune_res, metric = "rsq")
best_penalty

```

## Lasso model with optimised penalty/lambda value

We create the lasso regression workflow with the best penalty score.

```{r, message=FALSE}

lasso_final <- tune::finalize_workflow(x = lasso_workflow, 
                                       parameters = best_penalty)

lasso_final
```

We now train the lasso regression model with the training data

```{r, message=FALSE}

lasso_final_fit <- parsnip::fit(object = lasso_final, data = Hitters_train)
```

## Lasso regression model on test data

This lasso regression model can now be applied on our testing data set to validate its performance. For regression models, a .pred column is added.

```{r, message=FALSE}

test_results <- parsnip::augment(x = lasso_final_fit, new_data = Hitters_test)
  
test_results %>%
  reactable::reactable(defaultPageSize = 5)
  
```

```{r, message=FALSE}
test_results %>%
  yardstick::rsq(truth = .data[["Salary"]], estimate = .data[[".pred"]]) %>%
  reactable::reactable(defaultPageSize = 5)
```

# Principal Components Regression

The principal component regression is a linear model with pca transformed data. Hence, the major changes will be on the preprocessing steps.

## Create the resample object

```{r, message=FALSE}
set.seed(1234)
Hitters <- as_tibble(ISLR::Hitters) %>%
  tidyr::drop_na()

Hitters_split <- rsample::initial_split(Hitters, strata = "Salary")

Hitters_train <- rsample::training(Hitters_split)
Hitters_test <- rsample::testing(Hitters_split)

Hitters_fold <- rsample::vfold_cv(Hitters_train, v = 10)

```

## Create the preprocessor

We add [`recipes::step_pca`](https://recipes.tidymodels.org/reference/step_pca.html) this time to perfrom principal component analysis on all the predictors.
Threshold is the fraction of the total variance that should be covered by the components. 

For the below example, `threshold = .75` means that step_pca should generate enough components to capture 75 percent of the variability in the variables. Note: using this argument will override and reset any value given to `num_comp`.


```{r, message=FALSE}

pca_recipe <- 
  recipes::recipe(formula = Salary ~ ., data = Hitters_train) %>% 
  recipes::step_novel(recipes::all_nominal_predictors()) %>% 
  recipes::step_dummy(recipes::all_nominal_predictors()) %>% 
  recipes::step_zv(recipes::all_predictors()) %>% 
  recipes::step_normalize(recipes::all_predictors()) %>%
  recipes::step_pca(recipes::all_predictors(), threshold = 0.75)
```

```{r, message=FALSE}

rec <- recipes::prep(x = pca_recipe, training = Hitters_train)
  
rec %>%
  summary() %>%
  reactable::reactable(defaultPageSize = 5)

rec %>%
  recipes::bake(new_data = Hitters_train) %>%
  reactable::reactable(defaultPageSize = 5)

```

We will now try to find the best threshold value using the `tune::tune()`

```{r, message=FALSE}

pca_recipe <- 
  recipes::recipe(formula = Salary ~ ., data = Hitters_train) %>% 
  recipes::step_novel(recipes::all_nominal_predictors()) %>% 
  recipes::step_dummy(recipes::all_nominal_predictors()) %>% 
  recipes::step_zv(recipes::all_predictors()) %>% 
  recipes::step_normalize(recipes::all_predictors()) %>%
  recipes::step_pca(recipes::all_predictors(), threshold = tune::tune())
```

## Specify the model

We use a linear model

```{r, message=FALSE}

lm_spec <- 
  parsnip::linear_reg() %>% 
  parsnip::set_mode("regression") %>% 
  parsnip::set_engine("lm")

lm_spec %>%
  parsnip::translate()

```

## Create the workflow

```{r, message=FALSE}

pca_workflow <-  workflows::workflow() %>% 
  workflows::add_recipe(pca_recipe) %>% 
  workflows::add_model(lm_spec)

pca_workflow
```

## Create the threshold grid

A penalty/lambda grid of $10$ numbers from $0$ to $1$ is created.

```{r, message=FALSE}

# Create a range from 10
threshold_grid  <- dials::grid_regular(dials::threshold(range = c(0, 1)), levels = 10)

threshold_grid %>% 
  reactable::reactable(defaultPageSize = 5)
```

## Principal component regression model fitting on cross validated data

Now we have everything we need and we can fit all the models on the cross validated data. Note that this process may take some time.

```{r, message=FALSE, max.height='150px'}

tune_res <- tune::tune_grid(
  object = pca_workflow,
  resamples = Hitters_fold, 
  grid = threshold_grid
)

tune_res
```

Here we see how different threshold (Variance coverage) affects the performance metrics differently. Do note that using a different seed will give a different plot

```{r, message=FALSE}
# Note that a different seed will give different plots
tune::autoplot(tune_res)
```

The “best” values of this can be selected using `tune::select_best()`, this function requires you to specify a metric that it should select against. The threshold value is 0.889 for metric `rsme` since it gives the lowest value. Do note that using a different seed will give a different penalty/lambda value

```{r, message=FALSE, max.height='150px'}

top_threshold <- tune::show_best(tune_res, metric = "rmse", n = 5)
top_threshold

best_threshold <- tune::select_best(tune_res, metric = "rmse")
best_threshold

```

## Principal component regression model with optimised threshold value

We create the principal component regression workflow with the best threshold.

```{r, message=FALSE}

pca_final <- tune::finalize_workflow(x = pca_workflow, 
                                     parameters = best_threshold)

pca_final
```

We now train the lasso regression model with the training data

```{r, message=FALSE}

pca_final_fit <- parsnip::fit(object = pca_final, data = Hitters_train)
```

## Principal component regression model on test data

This principal component regression model can now be applied on our testing data set to validate its performance. For regression models, a .pred column is added.

```{r, message=FALSE}

test_results <- parsnip::augment(x = pca_final_fit, new_data = Hitters_test)
  
test_results %>%
  reactable::reactable(defaultPageSize = 5)
  
```

```{r, message=FALSE}
test_results %>%
  yardstick::rsq(truth = .data[["Salary"]], estimate = .data[[".pred"]]) %>%
  reactable::reactable(defaultPageSize = 5)
```

# Partial Least Square

The partial least square regression is a linear model with pls transformed data. Hence, the major changes will be on the preprocessing steps.

## Create the resample object

```{r, message=FALSE}
set.seed(1234)
Hitters <- as_tibble(ISLR::Hitters) %>%
  tidyr::drop_na()

Hitters_split <- rsample::initial_split(Hitters, strata = "Salary")

Hitters_train <- rsample::training(Hitters_split)
Hitters_test <- rsample::testing(Hitters_split)

Hitters_fold <- rsample::vfold_cv(Hitters_train, v = 10)

```

## Create the preprocessor

We add [`recipes::step_pls`](https://recipes.tidymodels.org/reference/step_pls.html) this time to perform partial least square calculation on all the predictors.
`num_comp` is the number of partial least square components to retain as new predictors.
`outcome` is the response variable for partial least square regression to use.

```{r, message=FALSE}

pls_recipe <- 
  recipes::recipe(formula = Salary ~ ., data = Hitters_train) %>% 
  recipes::step_novel(recipes::all_nominal_predictors()) %>% 
  recipes::step_dummy(recipes::all_nominal_predictors()) %>% 
  recipes::step_zv(recipes::all_predictors()) %>% 
  recipes::step_normalize(recipes::all_predictors()) %>%
  recipes::step_pls(recipes::all_predictors(), num_comp = 4, outcome = "Salary")
```

```{r, message=FALSE}

rec <- recipes::prep(x = pls_recipe, training = Hitters_train)
  
rec %>%
  summary() %>%
  reactable::reactable(defaultPageSize = 5)

rec %>%
  recipes::bake(new_data = Hitters_train) %>%
  reactable::reactable(defaultPageSize = 5)

```

We will now try to find the best threshold value using the `tune::tune()`

```{r, message=FALSE}

pls_recipe <- 
  recipes::recipe(formula = Salary ~ ., data = Hitters_train) %>% 
  recipes::step_novel(recipes::all_nominal_predictors()) %>% 
  recipes::step_dummy(recipes::all_nominal_predictors()) %>% 
  recipes::step_zv(recipes::all_predictors()) %>% 
  recipes::step_normalize(recipes::all_predictors()) %>%
  recipes::step_pls(recipes::all_predictors(), num_comp = tune::tune() , outcome = "Salary")
```
